# Resources

[My public Zotero library](https://www.zotero.org/valevo/library).

[Cultural AI's publica library](https://www.zotero.org/groups/2782799/cultural_ai).  


Andrei's resources on [GDrive](https://drive.google.com/drive/u/0/folders/1ncPfsOL_WmAUnGbEbqMMFAw1sF32gTLw)

Ryan's resources on [GitHub](https://github.com/ryanbrate/phd_reading_list)





## General Notes

- relatively little literature seems concerned with the task of detecting bias in corpora for the purpose of (synchronic) linguistic or sociological investigation
  or even for addressing **human** bias by making it visible; instead there seem to be two lines:
  
  - substantial efforts are starting to arise in quantifying, qualifying and mitigating bias for ML purposes, i.e. to address the bias that machines learn from humans and to try to develop machines that have as little bias as possible (and necessary, since learning without bias can be argued to be impossible) <br>
  here, there is some work on bias in corpora itself but only in the sense of corpora as training data
  
  - one more linguistically inclined line seems to be diachronic investigation of bias, i.e. to track how bias evolves through time; this is of course a subdiscipline of computational studies of the evolution of meaning



## People, Organisations et al

 - [Wayne Modest](https://www.materialculture.nl/en/about/wayne-modest) at _Material Culture_, editor of _Words Matter_


### Critical Stances in ML

 - Isabelle Augenstein: 
 - Timnit Gebru
 - Emily Bender
 - Margaret Mitchell


### Computers for LAM 

 - Thomas Padilla


### Philosophy & Social Sciences

 - [Markus Balkenhol](https://www.meertens.knaw.nl/cms/en/medewerkers/142839-markusb), Ethnologist at Meertens
 - [David Ludwig](http://david-ludwig.com/) from Wageningen University




## Blogs, Posts et al

 - [Text Embedding Models Contain Bias. Here's Why That Matters.](https://developers.googleblog.com/2018/04/text-embedding-models-contain-bias.html), a Google team's own investigation into the WEAT and surrounding issues

 - [The Sample Generator - Part 1: Origins](https://britishlibrary.typepad.co.uk/digital-scholarship/2013/11/the-sample-generator-part-1-origins.html?_ga=2.145789555.679492557.1611752125-107459358.1611318803), a post by the British Library on a tool to generate samples from collections for exemplar-based research <br>
  -> interesting in terms of bias, does the sampling algorithm contain, exacerbate <br>
  -> possible connection to the [idea that bias could be revealed via queries ](https://github.com/valevo/SABIO/tree/main/theory#bias-in-network-via-queries)

 - [Uncovering Unknown Unknowns in Machine Learning](https://ai.googleblog.com/2021/02/uncovering-unknown-unknowns-in-machine.html), a Google blog (by Lora Aroyo and others), a preliminary study into the *unknown unknowns* i.e. those weak spots we do not know about
 
 - [AI can be sexist and racist — it’s time to make it fair](https://www.nature.com/articles/d41586-018-05707-8) a comment on bias in AI, that is fairly unexciting (simply repeats many of the well-known phenomena and arguments); might even be a good example of how most of the efforts to tackle bias in AI are somewhat misguided

 - [Twitter post on showing bias in sentiment analysis](https://twitter.com/kareem_carr/status/1378881494370897921): good example for how sentiment analysis can be used to make bias visible; [reply by this Twitter post](https://twitter.com/DataSciBae/status/1379186952746889216) which explains why the original post contains misconceptions (and many agreeing)



