# Resources

Andrei's resources on [GDrive](https://drive.google.com/drive/u/0/folders/1ncPfsOL_WmAUnGbEbqMMFAw1sF32gTLw)

Ryan's resources on [GitHub](https://github.com/ryanbrate/phd_reading_list)


## General Notes

- relatively little literature seems concerned with the task of detecting bias in corpora for the purpose of (synchronic) linguistic or sociological investigation
  or even for addressing **human** bias by making it visible; instead there seem to be two lines:
  
  - substantial efforts are starting to arise in quantifying, qualifying and mitigating bias for ML purposes, i.e. to address the bias that machines learn from humans and to try to develop machines that have as little bias as possible (and necessary, since learning without bias can be argued to be impossible) <br>
  here, there is some work on bias in corpora itself but only in the sense of corpora as training data
  
  - one more linguistically inclined line seems to be diachronic investigation of bias, i.e. to track how bias evolves through time; this is of course a subdiscipline of computational studies of the evolution of meaning



## People, Organisations et al


### Critical Stances in ML

 - Isabelle Augenstein: 
 - Timnit Gebru
 - Emily Bender
 - Margaret Mitchell


### Computers for LAM 

 - Thomas Padilla


### Social Sciences

 - Markus Balkenhol








## Blogs, Posts et al

 - [Text Embedding Models Contain Bias. Here's Why That Matters.](https://developers.googleblog.com/2018/04/text-embedding-models-contain-bias.html), a Google team's own investigation into the WEAT and surrounding issues

 







