{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from utils import Ngram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Typicality\n",
    "\n",
    "The Typical set is defined as the set `S` of sentences in `\\Sigma*` s.t.\n",
    "        \n",
    "        H(\\Sigma*)-\\epsilon <= - 1/|S| log P(s_1, ..., s_{|S|}) <= H(\\Sigma*) + \\epsilon\n",
    "\n",
    "where we will assume for simplicity that the following identity is a good enough approximation\n",
    "\n",
    "        - log P(s_1, ..., s_{|S|}) = - \\sum_{s \\in S} log P(s)\n",
    "        \n",
    "Due to the chain rule, \n",
    "\n",
    "        P(s) = P(w1, ..., w_m) = \\prod_i P((w_i|w_1, ..., w_{i_1})\n",
    "        \n",
    "where we can approximate each term by\n",
    "\n",
    "        P(w_i| w_1, ..., w_{i-1}) = P(w_i|w_{i-n}, ..., w_{i-1})\n",
    "        \n",
    "which is also known as a `n`-gram model.\n",
    "\n",
    "\n",
    "Putting everything back together, we want to compute for each sentence `s = (w_1, ..., w_m)`\n",
    "\n",
    "        - \\sum_{i} log P(w_i | w_{i-n}, ..., w_{i-1})\n",
    "        \n",
    "and for a set of sentences S\n",
    "\n",
    "        - 1/|S| \\sum_{j \\in |S|} \\sum_{i \\in |S_j|} log P(w_ji | w_{ji-n}, ..., w_{ji-1})\n",
    "\n",
    "These two quantities are what we call the 'typicality' of a sentence and a set of sentences, respectively.\n",
    "\n",
    "\n",
    "\n",
    "### Ways to Improve\n",
    "\n",
    " - back-off smoothing: greedily start with `n` maximal, 'back off' to lower `n` if no counts available for a given `n`; this essentially the same as interpolating probabilities of longer sequences with shorter subsequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obj_tbl = pd.read_csv(\"../../data/tables/Objects.csv.gz\")\n",
    "# str_cols = \"Title\", \"Description\"\n",
    "# docs = [s for col in str_cols for s in obj_tbl[col].dropna()]\n",
    "docs = [\"hello world\", \"hasta la vista\", \"goede avond\", \"hasta la proxima\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)-grams: Term Document Matrix constructed...\n",
      "(1, 2)-grams: Init done\n"
     ]
    }
   ],
   "source": [
    "NG = Ngram(ngram_range=(1,2), documents=docs)\n",
    "\n",
    "ind_to_voc = {i: w for w, i in NG.vocab().items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 0, 1, 0, 0],\n",
       "        [0, 1, 0, 0, 1],\n",
       "        [1, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 1, 0]]),\n",
       " {'hello world': 2,\n",
       "  'hasta la': 1,\n",
       "  'la vista': 4,\n",
       "  'goede avond': 0,\n",
       "  'la proxima': 3})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NG.term_doc_matrix.toarray(), NG.vocab(with_inds=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def get_Ns():\n",
    "        l, h = NG.ngram_range\n",
    "        Ns = []\n",
    "        for n in range(l, h+1):\n",
    "            inds = list(NG.vocab(n, with_inds=True).values())\n",
    "            Ns.append(NG.term_doc_matrix[:, inds].sum())\n",
    "        return Ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRONG: need conditional probabilities (these are joint?) \n",
    "def sent_prob(sent_ind, ng, log=True):\n",
    "    _, inds = (ng.term_doc_matrix[sent_ind] > 0).nonzero()\n",
    "\n",
    "    reduce_f = np.sum if log else np.prod\n",
    "    \n",
    "    return reduce_f([NG.prob(ind_to_voc[word_ind], log=log) for word_ind in inds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_prob(sent_ind, ng, log=True):\n",
    "    _, inds = (ng.term_doc_matrix[sent_ind] > 0).nonzero()\n",
    "\n",
    "    reduce_f = np.sum if log else np.prod\n",
    "    \n",
    "    return reduce_f([NG.prob(ind_to_voc[word_ind], log=log) for word_ind in inds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = np.random.choice(len(docs), 1000)\n",
    "\n",
    "ps = [sent_prob(i, NG) for i in tqdm(inds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(ps)\n",
    "plt.xlim((-1000, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NG.cond_prob(\"la\", \"vista\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
