{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of docs:  11199\n",
      "number of tokens:  274580\n",
      "number of types:  43518\n",
      "TTR:  0.15848932915725836\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "obj_tbl = pd.read_csv(\"../data/tables/Objects.csv.gz\")\n",
    "str_cols = \"Title\", \"Description\"\n",
    "txts = [s for col in str_cols for s in obj_tbl[col].dropna()]\n",
    "\n",
    "print(\"number of docs: \", len(txts))\n",
    "tokens = [w for txt in txts for w in txt.split()]\n",
    "\n",
    "print(\"number of tokens: \", len(tokens))\n",
    "print(\"number of types: \", len(set(tokens)))\n",
    "print(\"TTR: \", len(set(tokens))/len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(2, 3)-grams: Padding documents...: 100%|██████████| 11199/11199 [00:00<00:00, 950061.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)-grams: Term Document Matrix constructed...\n",
      "(2, 3)-grams: Term frequencies precomputed...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(1, 2)-grams: Padding documents...: 100%|██████████| 11199/11199 [00:00<00:00, 1554026.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)-grams: Init done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)-grams: Term Document Matrix constructed...\n",
      "(1, 2)-grams: Term frequencies precomputed...\n",
      "(1, 2)-grams: Init done\n"
     ]
    }
   ],
   "source": [
    "NG3 = Ngram(3, documents=txts, padding=True, precompute_freqs=True)\n",
    "NG2 = Ngram(2, documents=txts, padding=True, precompute_freqs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IJZEREN PORTEMONNAIE MET LONTARBLAADJES 8.92936869363336e-05\n",
      "<s> ijzeren 0.0001008822313847876\n",
      "ijzeren portemonnaie 3.254265528541536e-06\n",
      "portemonnaie met 3.254265528541536e-06\n",
      "met lontarblaadjes 3.254265528541536e-06\n",
      "lontarblaadjes </s> 3.254265528541536e-06\n"
     ]
    }
   ],
   "source": [
    "print(txts[1], NG3.sent_prob(txts[1], log=False))\n",
    "\n",
    "for g in NG3.iter_ngrams(txts[1], 2):\n",
    "    print(g, NG3.prob(*g.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_pmis = {w.split()[1]: NG2.pmi(*w.split()) for w in NG2.vocab(2) if \"<s>\" in w}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115726/115726 [00:01<00:00, 72867.41it/s]\n"
     ]
    }
   ],
   "source": [
    "pmis = {w: NG2.pmi(*w.split()) for w in tqdm(NG2.vocab(2))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11199/11199 [00:03<00:00, 3339.66it/s]\n"
     ]
    }
   ],
   "source": [
    "pmis = [NG2.pmi(*w.split()) for t in tqdm(txts) for w in NG2.iter_ngrams(t, 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "284891\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(len(pmis))\n",
    "_ = plt.hist(pmis, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "NG2.save(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2]-grams: Init done\n"
     ]
    }
   ],
   "source": [
    "NG_ = Ngram.load(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_cv_params = dict(strip_accents=\"unicode\", lowercase=True, \n",
    "#                                      max_df=1., min_df=0., max_features=None,\n",
    "#                                      stop_words=None)\n",
    "# default_cv_params[\"token_pattern\"] = r\"(?u)\\b\\w\\w+\\b|\\<\\/s\\>|\\<s\\>\" #if not \"token_pattern\" in count_vectoriser_args else count_vectoriser_args[\"token_pattern\"]+r\"|\\<\\/s\\>|\\<s\\>\"\n",
    "        \n",
    "\n",
    "# cv = CountVectorizer(ngram_range=(1,2), **default_cv_params)\n",
    "# cv.build_analyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODOs\n",
    "\n",
    "\n",
    "features\n",
    " - stemming! spaCy's Dutch morphologizer perhaps\n",
    " - make PMI symmetric in its arguments (like its formal definition)? -> make that a choice, by adding a paramter to the functions\n",
    " \n",
    "bugs\n",
    " - analyzer will need to be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ngram:\n",
    "    def __repr__(self):\n",
    "        return (f\"{self.ns}-grams\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def _make_rng(ns):\n",
    "        if isinstance(ns, int):\n",
    "            return ns-1, ns\n",
    "        elif isinstance(ns, tuple) and len(ns) == 2:\n",
    "            return (ns[0]-1, ns[1])\n",
    "        else: raise ValueError(f\"{ns} is neither a number nor a tuple of two numbers!\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __init__(self, ns=None, documents=None, precompute_freqs=False, padding=True,\n",
    "                 term_doc_matrix=None, vocabulary=None, **count_vectoriser_args):\n",
    "#         self.n = n\n",
    "\n",
    "        default_cv_params = dict(strip_accents=\"unicode\", lowercase=True, \n",
    "                                     max_df=1., min_df=0., max_features=None,\n",
    "                                     stop_words=None)\n",
    "        default_cv_params.update(count_vectoriser_args)\n",
    "        default_cv_params[\"token_pattern\"] = r\"(?u)\\b\\w\\w+\\b|\\<\\/s\\>|\\<s\\>\" if not \"token_pattern\" in count_vectoriser_args else count_vectoriser_args[\"token_pattern\"]+r\"|\\<\\/s\\>|\\<s\\>\"\n",
    "        self.cv_params = default_cv_params\n",
    "\n",
    "        \n",
    "        if term_doc_matrix is not None:\n",
    "            if vocabulary is None: raise ValueError(\"term_doc_matrix supplied but no vocabulary!\")\n",
    "                \n",
    "            self.ns = sorted(set(map(lambda s: len(s.split()), vocabulary.keys())))\n",
    "            self.term_doc_matrix = term_doc_matrix\n",
    "            self.full_vocabulary = vocabulary\n",
    "            \n",
    "            nonce_cv = CountVectorizer(ngram_range=(1,2), **default_cv_params)\n",
    "            self.analyzer = nonce_cv.build_analyzer()\n",
    "         \n",
    "        \n",
    "        else:\n",
    "            if ns is None or documents is None: \n",
    "                raise ValueError(\"Need to supply both ngram_range and documents if nothing to load!\")\n",
    "            \n",
    "            self.ns = Ngram._make_rng(ns)\n",
    "            \n",
    "            if padding:\n",
    "                documents = [Ngram.pad_sentence(d, self.ns[1]-1) for d in tqdm(documents, desc=f\"{self}: Padding documents...\")]\n",
    "                \n",
    "            if vocabulary is not None:\n",
    "                default_cv_params[\"vocabulary\"] = vocabulary\n",
    "                # might need to add padding types to vocabulary \n",
    "        \n",
    "            vectoriser = CountVectorizer(ngram_range=self.ns, **default_cv_params)\n",
    "            \n",
    "            self.term_doc_matrix = vectoriser.fit_transform(documents)\n",
    "            self.full_vocabulary = vectoriser.vocabulary_\n",
    "            self.analyzer = vectoriser.build_analyzer()\n",
    "\n",
    "            print(f\"{self}: Term Document Matrix constructed...\")\n",
    "        \n",
    "        if precompute_freqs:\n",
    "            self.term_freqs = self.term_doc_matrix.sum(0)\n",
    "            print(f\"{self}: Term frequencies precomputed...\")\n",
    "        else:\n",
    "            self.term_freqs = None\n",
    "        \n",
    "        self.Ns = {i:self.get_N(i) for i in self.ns}\n",
    "        self.N = sum(self.Ns.values()) \n",
    "        print(f\"{self}: Init done\") \n",
    "\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def pad_sentence(sent, n, left=True, right=True):\n",
    "        if left:\n",
    "            sent = \"<s> \"*n + sent\n",
    "        if right:\n",
    "            sent = sent + \" </s>\"*n\n",
    "        return sent\n",
    "    \n",
    "    \n",
    "    lpad = re.compile(r\"^((\\<s\\> )+)\")\n",
    "    rpad = re.compile(r\"(( \\<\\/s\\>)+)$\")\n",
    "    @staticmethod\n",
    "    def unpad_sentence(sent):\n",
    "        sent = re.sub(Ngram.lpad, \"\", sent)\n",
    "        sent = re.sub(Ngram.rpad, \"\", sent)\n",
    "        return sent\n",
    "    \n",
    "         \n",
    "    def iter_ngrams(self, doc, n=None, padding=True, as_tuples=False):\n",
    "        if n is None:\n",
    "            n = self.ns[-1]\n",
    "            \n",
    "        if not n in range(self.ns[0], self.ns[1]+1):\n",
    "            raise ValueError(str(self) + \" not does not cover ngrams of length \" + str(n))\n",
    "        \n",
    "        if padding is True:\n",
    "            doc = Ngram.pad_sentence(doc, n - 1)\n",
    "        elif isinstance(padding, int):\n",
    "            doc = Ngram.pad_sentence(doc, padding)\n",
    "        else:\n",
    "            pass\n",
    "         \n",
    "        grams = self.analyzer(doc)\n",
    "        f = filter(lambda s: s.count(\" \") == (n-1), grams)\n",
    "        if not as_tuples:\n",
    "            return f\n",
    "        return map(str.split, f)\n",
    "    \n",
    "    \n",
    "    def get_N(self, n):\n",
    "        inds = list(self.vocab(n, with_inds=True).values())\n",
    "        return self.term_doc_matrix[:, inds].sum()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def freq(self, *w):\n",
    "        joined = \" \".join(w)\n",
    "        \n",
    "        if not joined in self.vocab():\n",
    "            raise ValueError(f\"'{joined}' not in {self}'s vocabulary\")\n",
    "\n",
    "        if self.term_freqs is not None:\n",
    "            return self.term_freqs[0, self.vocab()[joined]]\n",
    "        return self.term_doc_matrix[:, self.vocab()[joined]].sum()\n",
    "    \n",
    "    \n",
    "    def prob(self, *w, log=False):\n",
    "        if len(w) not in self.Ns:\n",
    "            raise ValueError(str(self) + \" not does not cover ngrams of length \" + str(len(w)))\n",
    "        \n",
    "        if not log:\n",
    "            return self.freq(*w)/self.Ns[len(w)]\n",
    "        \n",
    "        return np.log2(self.freq(*w)) - np.log2(self.N)\n",
    "    \n",
    "    \n",
    "    # let ws = (w_1, ..., w_k). computes p(w|w_1, ..., w_k) by #(<<w_1...w_k>w>)/#(<w_1...w_k>)\n",
    "    def cond_prob(self, w, *ws, log=False):\n",
    "        if not log:\n",
    "            return self.freq(*ws, w)/self.freq(*ws)\n",
    "        \n",
    "        return np.log2(self.freq(*ws, w)) - np.log2(self.freq(*ws))\n",
    "    \n",
    "    \n",
    "    \n",
    "    def sent_prob(self, sent, log=True, dont_reduce=False):\n",
    "        grams = list(self.iter_ngrams(sent, as_tuples=True))\n",
    "        reduce_f = np.sum if log else np.prod\n",
    "        if dont_reduce:\n",
    "            reduce_f = lambda ls: ls\n",
    "        return reduce_f([self.cond_prob(g[-1], *g[:-1], log=log) for g in grams])\n",
    "    \n",
    "    \n",
    "    def pmi(self, w1, w2):\n",
    "        return self.cond_prob(w2, w1, log=True) - self.prob(w2, log=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    def vocab(self, *ns, with_inds=False):\n",
    "        if not ns :\n",
    "            return self.full_vocabulary\n",
    "        \n",
    "        if not with_inds:\n",
    "            return {w for w in self.full_vocabulary if len(w.split()) in ns} \n",
    "        \n",
    "        return {w:i for w, i in self.full_vocabulary.items() if len(w.split()) in ns} \n",
    "    \n",
    "    \n",
    "    def save_matrix(self, path):\n",
    "        scipy.sparse.save_npz(path, self.term_doc_matrix)\n",
    "        \n",
    "    @staticmethod\n",
    "    def load_matrix(path):\n",
    "        return scipy.sparse.load_npz(path)\n",
    "    \n",
    "    \n",
    "    \n",
    "    sep = \"\\t\"\n",
    "    \n",
    "    \n",
    "    def save_vocab(self, path, *vocab_ns, with_inds=False):\n",
    "        sep = \"<||>\"\n",
    "        sep = \"\\t\"\n",
    "        \n",
    "        \n",
    "        vocab = self.vocab(*vocab_ns, with_inds=with_inds)\n",
    "        with open(path, \"w\") as handle:\n",
    "            rest = lambda w: (sep + str(vocab[w])) if with_inds else \"\"\n",
    "            for w in sorted(vocab):\n",
    "                handle.write(str(w) + rest(w))\n",
    "                handle.write(\"\\n\")        \n",
    "            \n",
    "#             if with_inds:\n",
    "#                 for w, i in vocab:\n",
    "#                     handle.write(str(w) + \"\\t\" + str(i))\n",
    "#                     handle.write(\"\\n\")\n",
    "#             else:\n",
    "#                 for w in vocab:\n",
    "#                     handle.write(str(w))\n",
    "#                     handle.write(\"\\n\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_vocab(path, with_inds=False):\n",
    "        with open(path) as handle:\n",
    "            lines = list(map(str.strip, handle))\n",
    "            for l in lines:\n",
    "                elems = l.split(\"\\t\")\n",
    "                if with_inds:\n",
    "                    yield elems[0], int(elems[1])\n",
    "                else:\n",
    "                    yield elems[0]\n",
    "\n",
    "                    \n",
    "    def save(self, name):\n",
    "        self.save_matrix(name + \".npz\")\n",
    "        self.save_vocab(name + \".vocab\", with_inds=True)\n",
    "        \n",
    "    @classmethod    \n",
    "    def load(cls, name, precompute_freqs=False):\n",
    "        td_mat = cls.load_matrix(name+\".npz\")\n",
    "        voc = dict(cls.load_vocab(name+\".vocab\", with_inds=True))\n",
    "        return cls(term_doc_matrix=td_mat, vocabulary=voc, precompute_freqs=precompute_freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('nl_core_news_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"Zowel sjamanen als hun vrouw dragen een hoofdsieraad van kralenwerk. Dit hoofdsieraad is met veren versierd, waaraan te zien is dat deze door de vrouw wordt gedragen. Het wordt tijdens ceremoniën gedragen in combinatie met een rotan hoofdband (zie 7086-14).\"\n",
    "\n",
    "s = \"\\n<>\\n\".join([s]*1000)\n",
    "\n",
    "\n",
    "\n",
    "doc = nlp(s)\n",
    "\n",
    "for sent in tqdm(doc.sents):\n",
    "    if \"<>\" in str(sent):\n",
    "        continue\n",
    "    for word in sent:\n",
    "        print(word.lemma_, end=\" \")\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
